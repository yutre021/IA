# IA
Here i gonna set a free material talking about AI and ML

Here is gonna be a json file with all learning, is gonna be talking AZURE, GEMINI and all
FEATURES OF AI


But lest start of history of ML

# 1950 - AI TIMELINE

<img src="assets/ai_timeline.png" alt="Texto alternativo" width="2000">

# 1943 A logical calculus of the ideas immanent in nervous activity
[Book in pdf](https://link.springer.com/article/10.1007/BF02478259)

# 1950 - Alan Turing

<img src="assets/alan.png" alt="Texto alternativo" width="2000">

 - [Article of the first thinking machine](https://archive.org/details/MIND--COMPUTING-MACHINERY-AND-INTELLIGENCE)

# Theory of Probability
- [PT Language](https://pt.wikipedia.org/wiki/Teoria_das_probabilidades)
 
 Fundamental of statistics was  build in century XVII.
<img src="assets/statistics.png" alt="Texto alternativo" width="2000">
 - The GOL: Analisys and model of data
 - [In 1800, Francis Galton e Karl Pearson. R. A. Fisher, Statistics Modern started](https://projecteuclid.org/journals/statistical-science/volume-7/issue-1/R-A-Fisher-The-Founder-of-Modern-Statistics/10.1214/ss/1177011442.full)


 # EDA Exploratory Data Analisys
 [EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis?utm_source=chatgpt.com)

 - In 1962, John Tukey, start the analisys exploration data, and bring a new era of ML
 [Buy the book](https://www.amazon.com/Exploratory-Data-Analysis-John-Tukey/dp/0201076160)
 - You can find the book free in alternative website, but recommended by the right way



# Introdu√ß√£o √† IA Generativa

Este README.md cont√©m um resumo conciso sobre conceitos fundamentais de Intelig√™ncia Artificial Generativa, perfeito para revis√£o r√°pida.

---

## O que √© um Prompt?

Um **prompt** √© um pequeno trecho de texto que voc√™ fornece a um **modelo de linguagem grande (LLM)** como entrada. Ele serve para controlar a sa√≠da do modelo de diversas maneiras, direcionando o que voc√™ deseja que ele gere.

---

## Exemplo de Modelo Generativo vs. Discriminativo

* Um **modelo generativo de IA** poderia ser treinado em um conjunto de dados de imagens de gatos e, em seguida, usado para **gerar novas imagens de gatos**. A ideia √© criar algo novo e semelhante aos dados de treinamento.
* Um **modelo discriminativo de IA** poderia ser treinado em um conjunto de dados de imagens de gatos e c√£es e, em seguida, usado para **classificar novas imagens como sendo de gatos ou c√£es**. A fun√ß√£o aqui √© distinguir entre diferentes categorias.

---

## O que √© IA Generativa?

**IA Generativa** √© um tipo de intelig√™ncia artificial que pode **criar conte√∫do novo e original**, como texto, imagens, √°udio e v√≠deo. Ela faz isso aprendendo a partir de dados existentes e, em seguida, usando esse conhecimento para produzir sa√≠das √∫nicas.

---

## O que s√£o Modelos de Funda√ß√£o em IA Generativa?

Um **modelo de funda√ß√£o** √© um **modelo de IA grande e abrangente** que √© pr√©-treinado em uma vasta quantidade de dados. Ele √© "projetado para ser adaptado" (ou ajustado/fine-tuned) para uma ampla gama de tarefas espec√≠ficas, como an√°lise de sentimento, legendagem de imagens e reconhecimento de objetos. Pense nele como uma base s√≥lida para diversas aplica√ß√µes de IA.

---

## O que Causa Alucina√ß√µes em Modelos de IA?

**Alucina√ß√µes** s√£o palavras ou frases geradas pelo modelo que muitas vezes s√£o sem sentido ou gramaticalmente incorretas. As principais causas s√£o:

* **O modelo n√£o recebe contexto suficiente:** Se o modelo n√£o tiver informa√ß√µes relevantes o bastante para a solicita√ß√£o, ele pode "inventar" detalhes.
* **O modelo n√£o foi treinado com dados suficientes:** Um conjunto de dados de treinamento muito pequeno ou pouco diversificado pode impedir o modelo de aprender padr√µes adequados, levando a respostas incoerentes.
* **O modelo √© treinado com dados ruidosos ou sujos:** Se os dados de treinamento contiverem imprecis√µes, vieses, inconsist√™ncias ou erros, o modelo pode aprender e reproduzir essas falhas em suas gera√ß√µes, resultando em alucina√ß√µes.

---

# Introduction to Generative AI

This README.md contains a concise summary of fundamental concepts in Generative Artificial Intelligence, perfect for a quick review.

---

## What is a Prompt?

A **prompt** is a short piece of text that you provide as input to a **large language model (LLM)**. It's used to control the model's output in many ways, directing what you want it to generate.

---

## Example of a Generative AI Model vs. a Discriminative AI Model

* A **generative AI model** could be trained on a dataset of cat images and then used to **generate new images of cats**. The goal here is to create something new that resembles the training data.
* A **discriminative AI model** could be trained on a dataset of cat and dog images and then used to **classify new images as either cats or dogs**. Its function is to distinguish between different categories.

---

## What is Generative AI?

**Generative AI** is a type of artificial intelligence that can **create new and original content**, such as text, images, audio, and video. It does this by learning from existing data and then using that knowledge to produce unique outputs.

---

## What are Foundation Models in Generative AI?

A **foundation model** is a **large AI model** that's pre-trained on a vast quantity of data. It's "designed to be adapted" (or fine-tuned) for a wide range of specific downstream tasks, such as sentiment analysis, image captioning, and object recognition. Think of it as a solid base for various AI applications.

---

## What Causes Hallucinations in AI Models?

**Hallucinations** are words or phrases generated by the model that are often nonsensical or grammatically incorrect. The main causes include:

* **The model isn't given enough context:** If the model lacks sufficient relevant information for the prompt, it might "fill in the blanks" with fabricated details.
* **The model isn't trained on enough data:** A training dataset that's too small or not diverse enough can prevent the model from learning proper patterns, leading to incoherent responses.
* **The model is trained on noisy or dirty data:** If the training data contains inaccuracies, biases, inconsistencies, or outright errors, the model can learn and reproduce these flaws in its generations, resulting in hallucinations.

## Document for Study


| Web site   | LINK                          |
| :---------- | :---------------------------------- |
| `Scikit-Learn` | http://scikit-learn.org/|


| Web site   | LINK                          |
| :---------- | :---------------------------------- |
| `TensorFlow` | https://tensorflow.org/|

| Web site   | LINK                          |
| :---------- | :---------------------------------- |
| `Keras` | https://keras.io/|




## Document

#### ML Supervised learning



| Web site   | LINK                          |
| :---------- | :---------------------------------- |
| `JMLR` | https://www.jmlr.org/?utm_source=chatgpt.com|


| Web site   | LINK                          |
| :---------- | :---------------------------------- |
| `Springer` | https://link.springer.com/article/10.1007/s42979-021-00592-x?utm_source=chatgpt.com|

| Web site   | LINK                          |
| :---------- | :---------------------------------- |
| `AZURE ML` | https://ml.azure.com|

| Web site   | LINK                          |
| :---------- | :---------------------------------- |
| `AZURE ML` | https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/01-machine-learning.html|

| Web site   | LINK                          |
| :---------- | :---------------------------------- |
| `AZURE AI` | https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/02-content-safety.html|

| Web site   | LINK                          |
| :---------- | :---------------------------------- |
| 'Tensorflow Playgroung` | https://playground.tensorflow.org/#activation=linear&regularization=L1&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.1&regularizationRate=0.01&noise=0&networkShape=4,2&seed=0.75500&showTestData=false&discretize=true&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false|



## üõ† Abilities
Python, R, Jupiter, and softwares of analysis...


 - [Python Documentation](https://docs.python.org/3/)
 - [R Documentationn](https://www.r-project.org/other-docs.html)
 - [Jupiter Documentationn](https://docs.jupyter.org/en/latest/)
 


## Learning

# Statistics

- Type of Data: Categorical and Numerical
- Categorical: YES OR NO, Logo of cars
- Numerical: Discrete ( Imagine each members of data set, number of children ) and Continues (Infinity and impossible to count, weight)

# Descriptive Statistics
- Qualitative is separete on Nominal and Ordinal
- Quantittive is separate in Interval and Ratio
- Mean, median, mode 
- Covariance and Correlation

## BASIC PROBABILITY
- Trial, Experiment , Experimental Probability, Expected Value
- Frequency
- Combinatorics (Permutations, Variations, Combinations)
- Factorials
- Bayesian Inference (Mutual Exclusive, Intersect, Union, Completely Overlap)
- Distributions (Discrete, Continuous)


The supervised machine, is when the machine is set data to mess with and find the best
solution to the target propose.

The unsupervised machine, is when the machine decid by data learned, by him self turn on new data 

Have one way that is the both sides, that human just give on sentence and the ML start the way, human can interfer but the computer give the last way


The Quantum Computing, gonna set a new Era of analysis data, my research about this is just in beginning

Reinforcement Learning.

AlphaGo (DeepMind)

[AlphaFold](https://deepmind.com/blog/article/alphafold)

[Quantum Microsoft](https://azure.microsoft.com/en-us/products/quantum/)

[Quantum Google](https://ai.google/research/teams/applied-science/quantum/)

[Quantum IBM](https://www.ibm.com/quantum-computing/)

[More of Quantum Computing](https://quantum.country/qcvc)

- On A neural network, is set by input layer, hiden layer, and output layer
- All layers is determined by maths functions

## BACKPROPAGATION

<img src="assets/backpropagation.png" alt="Texto alternativo" width="2000">

- There are Common activation functions like > Sigmoid (Logistic function) [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function), TanH(Hyperbolic tangent) [TanH](https://en.wikipedia.org/wiki/Hyperbolic_function#Hyperbolic_tangent), ReLu(rectified linear unit) [RedLu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) and [sofmat](https://en.wikipedia.org/wiki/Softmax_function)
  
- To see what to choose to plot a data [Data](https://www.data-to-viz.com/#heatmap),


# 1. Activation Functions
Activation functions introduce non-linearity into the network, allowing it to learn complex patterns. During backpropagation, the derivative of the activation function is used to compute the gradient.
Important Characteristics:
-	The activation function must be differentiable so that the gradient can be computed.
-	The choice of function can affect the learning performance.
Main Activation Functions:
1.	Sigmoid
-	Formula: f(x)=11+e‚àíxf(x) = \frac{1}{1 + e^{-x}}f(x)=1+e‚àíx1
-	Derivative: f‚Ä≤(x)=f(x)‚ãÖ(1‚àíf(x))f'(x) = f(x) \cdot (1 - f(x))f‚Ä≤(x)=f(x)‚ãÖ(1‚àíf(x))
-	Issues: Small gradients for extreme values (xxx) lead to vanishing gradient.
2.	Tanh
-	Formula: f(x)=ex‚àíe‚àíxex+e‚àíxf(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}f(x)=ex+e‚àíxex‚àíe‚àíx
-	Derivative: f‚Ä≤(x)=1‚àíf(x)2f'(x) = 1 - f(x)^2f‚Ä≤(x)=1‚àíf(x)2
-	Better than Sigmoid, but still suffers from small gradients for extreme values.
3.	ReLU (Rectified Linear Unit)
-	Formula: f(x)=max‚Å°(0,x)f(x) = \max(0, x)f(x)=max(0,x)
-	Derivative: f‚Ä≤(x)=1f'(x) = 1f‚Ä≤(x)=1 (if x>0x > 0x>0), 000 (if x‚â§0x \leq 0x‚â§0)
-	Issues: Dead neurons when many values become 0.
4.	Leaky ReLU
-	Introduces a small gradient for x<0x < 0x<0 (f‚Ä≤(x)=0.01f'(x) = 0.01f‚Ä≤(x)=0.01).
-	Solves the dead neuron issue.
5.	Softmax
-	Used in the output layer for multi-class classification.
-	Converts values into probabilities and works well with cross-entropy loss.
________________________________________
# 2. Loss Functions
The loss function measures the error between the network's predictions and the actual values. Backpropagation uses the gradient of the loss with respect to the weights to update them.
Important Characteristics:
-	The loss function should reflect the goal of the task (regression, classification, etc.).
-	The wrong choice can lead to problems such as underfitting or overfitting.
Main Loss Functions:
1.	Mean Squared Error (MSE)
-	Formula: L=1n‚àëi=1n(yi‚àíy^i)2L = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2L=n1‚àëi=1n(yi‚àíy^i)2
-	Used for regression.
-	Problem: Small gradients when errors are small, which can slow down learning.
2.	Mean Absolute Error (MAE)
-	Formula: L=1n‚àëi=1n‚à£yi‚àíy^i‚à£L = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|L=n1‚àëi=1n‚à£yi‚àíy^i‚à£
-	Resistant to outliers but can be less stable during optimization.
3.	Cross-Entropy Loss
-	Formula: L=‚àí‚àëi=1nyilog‚Å°(y^i)L = -\sum_{i=1}^n y_i \log(\hat{y}_i)L=‚àí‚àëi=1nyilog(y^i)
-	Used for classification, especially with Softmax.
-	Penalizes predictions with low confidence.
4.	Hinge Loss
-	Formula: L=max‚Å°(0,1‚àíy‚ãÖy^)L = \max(0, 1 - y \cdot \hat{y})L=max(0,1‚àíy‚ãÖy^)
-	Used for margin classifiers, like SVMs.
5.	Huber Loss
-	Combines MSE and MAE, being less sensitive to outliers.
-	Used for robust regression.
________________________________________
# Backpropagation: The Process
1.	Error calculation: The loss function evaluates the difference between the predicted output (y^\hat{y}y^) and the true value (yyy).
2.	Backward pass:
-	The gradient of the loss with respect to the final layer's outputs is calculated.
-	These gradients are propagated backward using the chain rule, considering the derivatives of the activation functions.
3.	Weight update:
-	Using the gradient and the optimizer (like SGD, Adam), the weights are adjusted.
________________________________________
# Relation and Choices
-	Activation functions affect how the gradient is propagated through the network. For example:
-	Sigmoid and Tanh can cause exploding/vanishing gradients.
-	ReLU and its variants help mitigate this.
- Loss functions influence the goal of learning:
- For classification: Cross-Entropy + Softmax.
-	For regression: MSE or Huber Loss.

# üìò Aplica√ß√µes de Derivadas e Integrais em Machine Learning

Este reposit√≥rio cont√©m anota√ß√µes e exemplos que mostram como **derivadas** e **integrais** s√£o aplicadas na pr√°tica em algoritmos de aprendizado de m√°quina.

---

## üìå 1. Derivadas na Otimiza√ß√£o ‚Äì Regress√£o Linear

Na **regress√£o linear**, o modelo tenta encontrar os melhores valores (pesos) que minimizam o erro entre as previs√µes e os valores reais.

A fun√ß√£o de erro usada geralmente √© o **Erro Quadr√°tico M√©dio (MSE)**, que mede a diferen√ßa entre o valor previsto e o real. O objetivo √© minimizar essa fun√ß√£o.

**Fun√ß√£o de custo simplificada (explicada):**
```
J(Œ∏) = (1 / 2m) * soma de todos os (previs√£o - valor real)¬≤
```

Para encontrar o valor ideal de Œ∏ (os pesos), usamos **Gradiente Descendente**:

**Regra de atualiza√ß√£o dos pesos:**
```
novo Œ∏ = Œ∏ - taxa_de_aprendizado * derivada do erro em rela√ß√£o a Œ∏
```

Essa derivada mostra a dire√ß√£o que o peso deve seguir para diminuir o erro.

---

## üìå 2. Derivadas no Backpropagation (Redes Neurais)

Em redes neurais, usamos derivadas para calcular como cada peso contribui para o erro final ‚Äî e ajust√°-los com base nisso.

**Etapas:**
1. A rede calcula o erro entre a sa√≠da prevista e a sa√≠da real.
2. Calcula a derivada da **fun√ß√£o de ativa√ß√£o**. Por exemplo, a fun√ß√£o **sigmoid**:
```
sigmoid(x) = 1 / (1 + e^-x)
derivada da sigmoid = sigmoid(x) * (1 - sigmoid(x))
```
3. A partir da camada de sa√≠da, o erro √© "retropropagado" para as camadas anteriores usando a **regra da cadeia**, ajustando todos os pesos.

---

## üìå 3. Integrais ‚Äì AUC, Probabilidades e Distribui√ß√µes Cont√≠nuas

### üîπ AUC (√Årea sob a Curva ROC)

A AUC √© uma m√©trica usada para avaliar classificadores bin√°rios. Ela representa a **√°rea sob a curva ROC**, que mostra a rela√ß√£o entre **falsos positivos e verdadeiros positivos**.

Embora a √°rea seja calculada numericamente, o conceito por tr√°s √© o mesmo de uma integral.

---

### üîπ Distribui√ß√µes Cont√≠nuas e Probabilidades

Em estat√≠stica, quando usamos distribui√ß√µes cont√≠nuas (como a **distribui√ß√£o normal**), a **probabilidade de um valor estar entre dois pontos** √© dada por uma integral da fun√ß√£o de densidade de probabilidade.

**Forma explicada:**
```
Probabilidade de X estar entre a e b = √°rea sob a curva da fun√ß√£o entre os pontos a e b
```

---

## üß™ Exemplo em Python: Gradiente Descendente

```python
import numpy as np
import matplotlib.pyplot as plt

# Fun√ß√£o que queremos minimizar: f(x) = x¬≤
def f(x):
    return x**2

# Derivada da fun√ß√£o: f'(x) = 2x
def df(x):
    return 2 * x

# Ponto inicial
x = 10
alpha = 0.1  # Taxa de aprendizado
points = []

# Gradiente descendente
for _ in range(20):
    x = x - alpha * df(x)  # Atualiza x
    points.append(x)

# Visualizando a descida
x_vals = np.linspace(-10, 10, 100)
plt.plot(x_vals, f(x_vals), label='f(x) = x¬≤')
plt.scatter(points, [f(p) for p in points], color='red', label='Passos do Gradiente')
plt.title("Gradiente Descendente")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.legend()
plt.grid()
plt.show()
```

# Cluster
- Is for a process of a lot of amount data, where are distribution for others computer. [Cluster](https://www.ibm.com/docs/pt-br/ibm-mq/9.2?topic=management-application-programming-clusters&utm_source=chatgpt.com)

- [Cluster 2](https://arxiv.org/abs/2207.05677?utm_source=chatgpt.com)


# Linear Regression
- [Linear Regression](https://github.com/yutre021/IA/blob/main/linerar_regression.ipynb)
# RNA
- BUILDED OF PREDEFINED NEURONS
- Computional Neurons
- N -> IN
<img src="assets/RNA2.png" alt="Texto alternativo" width="2000">

# Perceptron
- It's a simple code math that calculate one weighted sum
- [Here is one simple code example](https://github.com/yutre021/IA/blob/main/perceptron.py)
- 1958: Frank Rosenblatt introduced the Perceptron, inspired by the human brain, to recognize patterns.
- Initial Success: Gained attention as a machine learning breakthrough.
- 1969: Minsky and Papert proved its limitation to solving only linearly separable problems (e.g., failed on XOR).
- AI Winter: Interest in neural networks declined after this critique.
- 1980s Revival: Multi-layer perceptrons and backpropagation brought neural networks back into focus.
- Legacy: Perceptron remains a foundational concept in machine learning history.

# Supervised Learning
- Algorithm K
- Algorithm Linear Regression
- Algorithm Logistic
- SVMs
- Neural Network

# Unsupervised Learning
- K-Means
- DBSCAN
- HCA
- One-class SVM
- KERNEL ACP
- LLW
- t-SNE
- Eclat

# XOR Problem - Neural Network with Sigmoid Activation

## Overview
[View the XOR Neural Network Code](https://github.com/yutre021/IA/blob/main/xor_problem.cpp)

This project implements a basic neural network for solving the **XOR (Exclusive OR)** problem. XOR is a binary operation that returns:
- `1` if the two inputs are different.
- `0` if the two inputs are the same.

A neural network with **sigmoid activation** is used to classify the XOR outputs based on the two input variables.

## Sigmoid Function

The **sigmoid function** is a commonly used **activation function** in neural networks. It takes a real-valued input and maps it to an output between 0 and 1. This makes it ideal for binary classification tasks, like the XOR problem, where the output needs to be in the range of 0 or 1.

The **sigmoid function** is defined as:

\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

Where:
- \(x\) is the weighted sum of inputs to a neuron.
- \(e\) is Euler's number (approximately 2.71828).

The sigmoid function is continuous and smooth, which is why it's a preferred activation function in many cases. For the XOR problem, it helps the network adjust the output values to approximate the correct binary outputs (0 or 1).

### Why Sigmoid in XOR?

The XOR problem cannot be solved by a simple linear model. A neural network, using sigmoid as an activation function, can model this non-linear relationship. The network will learn to output values close to `0` and `1`, depending on the input combinations.

By training the network, the model adjusts the weights to fit the XOR logic, where:
- Input `(0, 0)` should output `0`
- Input `(0, 1)` should output `1`
- Input `(1, 0)` should output `1`
- Input `(1, 1)` should output `0`

## How It Works

1. **Initialization:**
   - The network is initialized with random weights for the inputs and hidden layers.
   
2. **Forward Propagation:**
   - For each input pair (from the XOR truth table), the network computes the weighted sum of inputs, applies the sigmoid activation function, and outputs a value.

3. **Training:**
   - The network adjusts its weights based on the error between the predicted output and the target output using backpropagation (not shown in this code, but an essential part of training neural networks).

4. **Sigmoid Activation:**
   - The sigmoid function is used in the hidden layers to map the weighted sum of inputs to an output in the range of 0 to 1.

## Running the Code

To run the XOR neural network:

1. Clone this repository to your local machine.
2. Compile and run the C++ code using any C++ compiler (e.g., g++).
3. Input the values for `lX`, `lY`, `wX`, and `wY` when prompted, and the network will process these values using the sigmoid activation function.

```bash
g++ xor_neural_network.cpp -o xor_nn
./xor_nn
```
## BIBLIOGRAPHY

Bibliografia B√°sica

STEINBRUCH, A; WINTERLE, Paulo. √ÅLGEBRA LINEAR. 2. ed. S√£o Paulo: Pearson, 2014.

BRUCE, P; BRUCE, A. ESTAT√çSTICA PR√ÅTICA: para cientistas de dados. 1. ed. Rio de Janeiro: Alta Books, 2019.

GONZALEZ, Rafael C; WOODS, Richard E. Processamento digital de imagens. 3. ed. S√£o Paulo: Pearson, 2010.

GONZALEZ, R. C.; WOODS, R. E. Processamento de Imagens Digitais. S√£o Paulo: Edgard Bl√ºcher, 2000.

PEDRINI, H√©lio; SCHWARTZ, William Robson. An√°lise de imagens digitais: princ√≠pios, algoritmos e aplica√ß√µes. S√£o Paulo: Thomson Learning, 2008.

CASTLEMAN, Kenneth R. Digital image processing. Upper Saddle River, N. J.: Prentice-Hall, 1996.

FOLEY, J. D. Computer Graphics: Principles and Practices. 2nd ed. New York: Addison-Wesley, 1996.

Bibliografia Complementar

HEINY, Loren. Advanced graphics programming using C/C++. New York: John Wiley & Sons, 1993.

LYON, Douglas A. Image processing in Java. Upper Saddle River,NJ: Prentice-Hall, 1999

MATSUMOTO, Elia Yathie. MATLAB 7: fundamentos. S√£o Paulo: √ârica, 2004








## Author

- [@yuri](https://github.com/yutre021)


## üöÄ About me
I'm Computer Engeneering, with focous in AI ENGENEERING



## Roadmap

- Searchs 2025

- Get in Quantum Computing 


## Get in better

- If you have any sugestion just adm me



